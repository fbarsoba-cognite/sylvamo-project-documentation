"""
Fabric Delta Table Extractor for CDF RAW
=========================================
64-bit Python alternative to the 32-bit Cognite Fabric Connector.
Reads Delta tables from Microsoft Fabric OneLake -> CDF RAW.

Usage:
    python fabric_delta_extractor.py --config configs/ppr-hist-roll.yaml
    python fabric_delta_extractor.py --config configs/ppr-hist-roll.yaml --poll
    python fabric_delta_extractor.py --config configs/ppr-hist-roll.yaml --dry-run

Generated by Setup-FabricExtractors.ps1 | Jira: SVQS-160
"""

import argparse, gc, hashlib, json, logging, os, sys, time
from datetime import datetime, timezone
from pathlib import Path
import yaml


def get_available_memory_mb():
    """Return available physical memory in MB (Windows-specific, fallback for others)."""
    try:
        import ctypes
        class MEMORYSTATUSEX(ctypes.Structure):
            _fields_ = [
                ("dwLength", ctypes.c_ulong),
                ("dwMemoryLoad", ctypes.c_ulong),
                ("ullTotalPhys", ctypes.c_ulonglong),
                ("ullAvailPhys", ctypes.c_ulonglong),
                ("ullTotalPageFile", ctypes.c_ulonglong),
                ("ullAvailPageFile", ctypes.c_ulonglong),
                ("ullTotalVirtual", ctypes.c_ulonglong),
                ("ullAvailVirtual", ctypes.c_ulonglong),
                ("ullAvailExtendedVirtual", ctypes.c_ulonglong),
            ]
        stat = MEMORYSTATUSEX()
        stat.dwLength = ctypes.sizeof(stat)
        ctypes.windll.kernel32.GlobalMemoryStatusEx(ctypes.byref(stat))
        return stat.ullAvailPhys // (1024 * 1024)
    except Exception:
        return None  # Non-Windows or error


def adaptive_batch_size(default_batch_size, logger=None):
    """Reduce batch size if available memory is low."""
    log = logger or logging.getLogger(__name__)
    avail_mb = get_available_memory_mb()
    if avail_mb is None:
        return default_batch_size
    log.info(f"Available memory: {avail_mb} MB")
    if avail_mb < 500:
        new_size = min(default_batch_size, 5000)
        log.warning(f"Low memory ({avail_mb} MB)! Reducing batch size to {new_size}")
        return new_size
    elif avail_mb < 1000:
        new_size = min(default_batch_size, 10000)
        log.warning(f"Memory below 1GB ({avail_mb} MB), reducing batch size to {new_size}")
        return new_size
    elif avail_mb < 2000:
        new_size = min(default_batch_size, 25000)
        log.info(f"Memory below 2GB ({avail_mb} MB), reducing batch size to {new_size}")
        return new_size
    return default_batch_size

LOG_FORMAT = "%(asctime)s UTC [%(levelname)-8s] %(name)s - %(message)s"
logging.Formatter.converter = time.gmtime


def setup_logging(log_file=None, console_level="INFO"):
    logger = logging.getLogger("fabric_delta_extractor")
    logger.setLevel(logging.DEBUG)
    ch = logging.StreamHandler()
    ch.setLevel(getattr(logging, console_level.upper(), logging.INFO))
    ch.setFormatter(logging.Formatter(LOG_FORMAT))
    logger.addHandler(ch)
    if log_file:
        log_path = Path(log_file)
        log_path.parent.mkdir(parents=True, exist_ok=True)
        fh = logging.FileHandler(log_path, encoding="utf-8")
        fh.setLevel(logging.DEBUG)
        fh.setFormatter(logging.Formatter(LOG_FORMAT))
        logger.addHandler(fh)
    return logger


def load_config(config_path):
    with open(config_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def parse_config(cfg):
    cognite = cfg.get("cognite", {})
    idp = cognite.get("idp-authentication", {})
    extractor = cfg.get("extractor", {})
    source = cfg.get("source", {})
    raw_tables = source.get("raw-tables", [])
    logger_cfg = cfg.get("logger", {})
    if not raw_tables:
        raise ValueError("No raw-tables defined in config")
    table = raw_tables[0]
    state_store = extractor.get("state-store", {})
    local_state = state_store.get("local", {})
    return {
        "project": cognite.get("project"),
        "host": cognite.get("host"),
        "tenant": idp.get("tenant"),
        "client_id": idp.get("client-id"),
        "secret": idp.get("secret"),
        "scopes": idp.get("scopes", []),
        "abfss_prefix": source.get("abfss-prefix"),
        "data_set_id": source.get("data-set-id"),
        "read_batch_size": source.get("read_batch_size", 50000),
        "raw_table": table.get("table-name"),
        "raw_db": table.get("db-name"),
        "raw_path": table.get("raw-path"),
        "md5_key": table.get("md5-key", False),
        "incremental_field": table.get("incremental-field"),
        "poll_time": extractor.get("poll-time", 3600),
        "ingest_batch_size": extractor.get("ingest-batch-size", 10000),
        "state_path": local_state.get("path", "state/extractor-state.json"),
        "console_level": logger_cfg.get("console", {}).get("level", "INFO"),
        "log_file": logger_cfg.get("file", {}).get("path"),
    }


class StateStore:
    def __init__(self, path):
        self.path = Path(path)
        self.path.parent.mkdir(parents=True, exist_ok=True)
        self._state = self._load()

    def _load(self):
        if self.path.exists():
            try:
                with open(self.path, "r", encoding="utf-8") as f:
                    return json.load(f)
            except (json.JSONDecodeError, OSError):
                return {}
        return {}

    def get(self, key, default=None):
        return self._state.get(key, default)

    def set(self, key, value):
        self._state[key] = value

    def save(self):
        with open(self.path, "w", encoding="utf-8") as f:
            json.dump(self._state, f, indent=2, default=str)


def read_delta_table(abfss_prefix, raw_path, tenant, client_id, secret,
                     batch_size=50000, incremental_field=None,
                     incremental_value=None, logger=None):
    from deltalake import DeltaTable
    log = logger or logging.getLogger(__name__)
    table_uri = f"{abfss_prefix}/{raw_path}"
    log.info(f"Opening Delta table: {table_uri}")
    storage_options = {
        "azure_client_id": client_id,
        "azure_client_secret": secret,
        "azure_tenant_id": tenant,
        "azure_storage_account_name": "onelake",
    }
    dt = DeltaTable(table_uri, storage_options=storage_options)
    schema = dt.schema()
    try:
        file_uris = dt.file_uris()
        n_files = len(file_uris)
    except AttributeError:
        n_files = 0
    log.info(f"Delta table: {len(schema.fields)} columns, {n_files} files")
    log.debug(f"Columns: {[f.name for f in schema.fields]}")
    # Estimate total rows from Parquet metadata for progress tracking
    estimated_rows = 0
    try:
        import pyarrow.parquet as pq
        dataset = dt.to_pyarrow_dataset()
        for frag in dataset.get_fragments():
            try:
                md = frag.metadata
                if md:
                    estimated_rows += md.num_rows
            except Exception:
                pass
        if estimated_rows > 0:
            log.info(f"Estimated total rows: {estimated_rows:,}")
    except Exception:
        pass
    scanner_kwargs = {"batch_size": batch_size}
    if incremental_field and incremental_value:
        import pyarrow.dataset as ds
        log.info(f"Incremental filter: {incremental_field} > '{incremental_value}'")
        scanner_kwargs["filter"] = ds.field(incremental_field) > incremental_value
        estimated_rows = 0  # Can't estimate with filters
    if not estimated_rows:
        dataset = dt.to_pyarrow_dataset()
    scanner = dataset.scanner(**scanner_kwargs)
    batch_num = 0
    total_rows = 0
    start_time = time.time()
    for batch in scanner.to_batches():
        batch_num += 1
        total_rows += len(batch)
        elapsed = time.time() - start_time
        rate = total_rows / elapsed if elapsed > 0 else 0
        if estimated_rows > 0:
            pct = min(total_rows / estimated_rows * 100, 99.9)
            remaining = (estimated_rows - total_rows) / rate if rate > 0 else 0
            eta_min = remaining / 60
            log.info(f"Batch {batch_num}: {len(batch):,} rows | "
                     f"{total_rows:,}/{estimated_rows:,} ({pct:.1f}%) | "
                     f"{rate:,.0f} rows/s | ETA: {eta_min:.1f}m")
        else:
            log.info(f"Batch {batch_num}: {len(batch):,} rows | "
                     f"total: {total_rows:,} | {rate:,.0f} rows/s")
        yield batch
    elapsed = time.time() - start_time
    rate = total_rows / elapsed if elapsed > 0 else 0
    log.info(f"Finished reading: {total_rows:,} rows in {batch_num} batches "
             f"({elapsed:.1f}s, {rate:,.0f} rows/s)")


def make_row_key(row, md5_key):
    if md5_key:
        values_str = "|".join(str(v) if v is not None else "" for v in row.values())
        return hashlib.md5(values_str.encode("utf-8")).hexdigest()
    return ""


def _json_safe(val):
    """Convert non-JSON-serializable types to strings."""
    if val is None:
        return val
    from datetime import datetime, date, time
    from decimal import Decimal
    if isinstance(val, datetime):
        return val.isoformat()
    if isinstance(val, date):
        return val.isoformat()
    if isinstance(val, time):
        return val.isoformat()
    if isinstance(val, Decimal):
        return float(val)
    if isinstance(val, bytes):
        return val.hex()
    return val


def batch_to_raw_rows(batch, md5_key, row_offset=0):
    """Convert a PyArrow batch to CDF RAW rows without creating an extra Table copy."""
    rows = []
    columns = batch.schema.names
    num_rows = batch.num_rows
    # Access columns directly from the batch (avoids pa.Table.from_batches copy)
    col_arrays = {col: batch.column(col) for col in columns}
    for i in range(num_rows):
        row_dict = {col: _json_safe(col_arrays[col][i].as_py()) for col in columns}
        key = make_row_key(row_dict, md5_key=True) if md5_key else str(row_offset + i)
        rows.append({"key": key, "columns": row_dict})
    del col_arrays  # Free column references
    return rows


def write_to_cdf_raw(client, db_name, table_name, rows, ingest_batch_size=10000,
                     logger=None, dry_run=False):
    log = logger or logging.getLogger(__name__)
    if dry_run:
        log.info(f"[DRY RUN] Would write {len(rows):,} rows to {db_name}.{table_name}")
        return
    try:
        client.raw.databases.create(db_name)
    except Exception:
        pass  # Already exists
    try:
        client.raw.tables.create(db_name, table_name)
    except Exception:
        pass  # Already exists
    total = len(rows)
    n_chunks = (total + ingest_batch_size - 1) // ingest_batch_size
    for chunk_idx, i in enumerate(range(0, total, ingest_batch_size), 1):
        chunk = rows[i : i + ingest_batch_size]
        from cognite.client.data_classes import Row
        raw_rows = [Row(key=r["key"], columns=r["columns"]) for r in chunk]
        client.raw.rows.insert(db_name, table_name, raw_rows, ensure_parent=True)
        pct = (i + len(chunk)) / total * 100
        log.info(f"  Upload [{chunk_idx}/{n_chunks}]: {i + len(chunk):,}/{total:,} ({pct:.0f}%)")
    log.info(f"Written {total:,} rows to {db_name}.{table_name}")


def create_cdf_client(project, host, tenant, client_id, secret, scopes):
    from cognite.client import CogniteClient, ClientConfig
    from cognite.client.credentials import OAuthClientCredentials
    credentials = OAuthClientCredentials(
        token_url=f"https://login.microsoftonline.com/{tenant}/oauth2/v2.0/token",
        client_id=client_id, client_secret=secret, scopes=scopes,
    )
    config = ClientConfig(
        client_name="fabric-delta-extractor",
        project=project, credentials=credentials, base_url=host,
    )
    return CogniteClient(config)


def run_extraction(params, dry_run=False, poll=False):
    logger = setup_logging(log_file=params["log_file"], console_level=params["console_level"])
    logger.info("=" * 60)
    logger.info("Fabric Delta Extractor (64-bit Python) [memory-optimized]")
    logger.info(f"Table: {params['raw_db']}.{params['raw_table']}")
    logger.info(f"Source: {params['abfss_prefix']}/{params['raw_path']}")
    # Adaptive batch size based on available memory
    batch_size = adaptive_batch_size(params["read_batch_size"], logger=logger)
    logger.info(f"Batch: {batch_size} rows | MD5: {params['md5_key']}")
    logger.info(f"Incremental: {params['incremental_field'] or 'None (full load)'}")
    logger.info(f"Python: {sys.version} ({'64-bit' if sys.maxsize > 2**32 else '32-bit!'})")
    logger.info("=" * 60)
    if sys.maxsize <= 2**32:
        logger.warning("32-bit Python detected! Install 64-bit Python for large tables.")
    state = StateStore(params["state_path"])
    state_key = f"{params['raw_db']}.{params['raw_table']}"
    client = None
    if not dry_run:
        logger.info("Connecting to CDF...")
        client = create_cdf_client(
            project=params["project"], host=params["host"], tenant=params["tenant"],
            client_id=params["client_id"], secret=params["secret"], scopes=params["scopes"],
        )
        logger.info(f"Connected to CDF project: {params['project']}")
    while True:
        try:
            run_start = time.time()
            incremental_value = None
            if params["incremental_field"]:
                incremental_value = state.get(f"{state_key}.high_watermark")
                if incremental_value:
                    logger.info(f"Resuming from: {params['incremental_field']} > '{incremental_value}'")
                else:
                    logger.info("No high watermark, doing full load")
            total_rows = 0
            max_incr = incremental_value
            row_offset = 0
            batch_num = 0
            for batch in read_delta_table(
                abfss_prefix=params["abfss_prefix"], raw_path=params["raw_path"],
                tenant=params["tenant"], client_id=params["client_id"],
                secret=params["secret"], batch_size=batch_size,
                incremental_field=params["incremental_field"],
                incremental_value=incremental_value, logger=logger,
            ):
                batch_num += 1
                rows = batch_to_raw_rows(batch, md5_key=params["md5_key"], row_offset=row_offset)
                batch_len = len(rows)
                row_offset += batch_len
                if params["incremental_field"]:
                    import pyarrow.compute as pc
                    col = batch.column(params["incremental_field"])
                    batch_max = pc.max(col).as_py()
                    if batch_max is not None:
                        batch_max_str = str(batch_max)
                        if max_incr is None or batch_max_str > max_incr:
                            max_incr = batch_max_str
                # Free the PyArrow batch before uploading (no longer needed)
                del batch
                write_to_cdf_raw(
                    client=client, db_name=params["raw_db"],
                    table_name=params["raw_table"], rows=rows,
                    ingest_batch_size=params["ingest_batch_size"],
                    logger=logger, dry_run=dry_run,
                )
                total_rows += batch_len
                # Explicit memory cleanup after each batch
                del rows
                gc.collect()
                # Log memory status periodically
                if batch_num % 5 == 0:
                    avail = get_available_memory_mb()
                    if avail is not None:
                        logger.info(f"Memory check: {avail} MB available (after batch {batch_num})")
                        if avail < 300:
                            logger.warning(f"Critical memory: {avail} MB! Forcing GC...")
                            gc.collect()
                            time.sleep(5)  # Brief pause for OS to reclaim
                if max_incr and params["incremental_field"] and not dry_run:
                    state.set(f"{state_key}.high_watermark", max_incr)
                    state.save()
            if not dry_run:
                state.set(f"{state_key}.last_run", datetime.now(timezone.utc).isoformat())
                state.set(f"{state_key}.last_row_count", total_rows)
            state.save()
            elapsed = time.time() - run_start
            rate = f" ({total_rows / elapsed:.0f} rows/s)" if elapsed > 0 else ""
            logger.info(f"Extraction complete: {total_rows} rows in {elapsed:.1f}s{rate}")
            # Final memory cleanup
            gc.collect()
        except KeyboardInterrupt:
            logger.info("Interrupt received, stopping gracefully")
            break
        except Exception:
            logger.exception("Extraction failed")
            if not poll:
                raise
        if not poll:
            break
        logger.info(f"Sleeping {params['poll_time']}s until next poll...")
        try:
            time.sleep(params["poll_time"])
        except KeyboardInterrupt:
            logger.info("Interrupt during sleep, stopping")
            break


def main():
    parser = argparse.ArgumentParser(description="Extract Delta tables from Fabric OneLake to CDF RAW (64-bit)")
    parser.add_argument("--config", required=True, help="Fabric connector YAML config file")
    parser.add_argument("--poll", action="store_true", help="Run continuously with polling")
    parser.add_argument("--dry-run", action="store_true", help="Read only, don't write to CDF")
    args = parser.parse_args()
    cfg = load_config(args.config)
    params = parse_config(cfg)
